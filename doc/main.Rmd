---
title: "Project 3"
author: "Yunfan Li, Mingyu Yang, Peiqi Jin, Shichao Jia, Yanchen Chen"
output: html_notebook
---


```{r}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}

if(!require("gbm")){
  install.packages("gbm")
}

library("EBImage")
library("gbm")

if(!require("xgboost")){
  install.packages("xgboost")
}

if(!require("plotly")){
  install.packages("plotly")
}

library("EBImage")
library("gbm")
library("xgboost")
library("plotly")
```


### Step 0: specify directories.

Set the working directory to the image folder.In order to obain reproducible results, set.seed() whenever randomization is used. 

```{r wkdir, eval=FALSE}
set.seed(2018)
setwd("~/Documents/GitHub/Fall2018-Proj3-Sec1-grp6")

```

Provide directories for training images. Low-resolution (LR) image set and High-resolution (HR) image set will be in different subfolders. 
```{r}
train_dir <- "../data/train_set/" # This will be modified for different data sets.


train_LR_dir <- paste(train_dir, "LR/", sep="")
train_HR_dir <- paste(train_dir, "HR/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="") 

```

### Step 1: set up controls for evaluation experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

```{r exp_setup}
run.cv=TRUE # run cross-validation on the training set
K <- 3  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
```

Using cross-validation or independent test set evaluation, we compare the performance of models with different specifications. In the baseline, we use GBM with different `depth` and `trees`.In the advanced model, we use XGBoost with different `max_depth`, `eta`, `subsample`, `min_child_weight`. 
In the following chunk,we list, `depth` and `trees` corresponding to GBM model that we will compare.
Also, we list `max_depth`, `eta`, `subsample`, `min_child_weight` corresponding to Xgboost model that we will compare.

```{r model_setup}
model_values <- expand.grid(depth = c(9, 13), n.trees = c(250,300))
model_values_xgb <- expand.grid(max_depth = c(4,6,8), eta = c(0.1,0.3,0.5), subsample = c(0.5,0.8), min_child_weight = c(4,6,8))

```

### Step 2: import training images class labels.

We provide extra information of image label: car (0), flower (1), market (2). These labels are not necessary for your model.

```{r train_label}
#extra_label <- read.csv(train_label_path, colClasses=c("NULL", NA, NA))
```

### Step 3: construct features and responses

`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. 
+ `feature.R`
  + Input: a path for low-resolution images.
  + Input: a path for high-resolution images.
  + Output: an RData file that contains extracted features and corresponding responses

```{r feature}
#setwd("~/Documents/GitHub/Fall2018-Proj3-Sec1-grp6")
source("../lib/feature.R")

tm_feature_train <- NA
if(run.feature.train){
  tm_feature_train <- system.time(dat_train <- feature(train_LR_dir, train_HR_dir))
  feat_train <- dat_train$feature
  label_train <- dat_train$label
}

#save(dat_train, file="./output/feature_train.RData")
```


### Step 4: Train a classification model with training images 
Call the train model and test model from library. 

Model1: GBM
```{r loadlib}
source("../lib/train.R")
source("../lib/test.R")
```

#### Model selection with cross-validation (GBM)
* Do model selection by choosing among different values of training model parameters, that is, the interaction depth for GBM in this example. 
```{r runcv GBM, echo=TRUE, message=FALSE, warning=FALSE, results = "hide"}
source("../lib/cross_validation.R")

if(run.cv){
  err_cv <- array(dim=c(nrow(model_values), 2))
  for(k in 1:nrow(model_values)){
    cat("k=", k, "\n")
    err_cv[k,] <- cv.function(feat_train, label_train, model_values[k, ], K)
  }
  save(err_cv, file="../output/err_cv.RData")
}


```

Visualize cross-validation results. 
```{r cv_vis gbm, echo=FALSE, message=FALSE, warning=FALSE}

if(run.cv){
  #load("../output/err_cv.RData")
  mv <- factor(paste("(", model_values$depth,",", model_values$n.trees, ")"))
  plot(err_cv[,1]~mv, xlab="(Interaction Depth, Number of trees)", ylab="CV Error",
       main="Cross Validation Error", type="n", ylim=c(0, 0.005))
  points(err_cv[,1]~mv, col="blue", pch=16)
  lines(err_cv[,1]~mv, col="blue")
  #arrows(mv,err_cv[,1]-err_cv[,2], mv, err_cv[,1]+err_cv[,2], 
        #length=0.1, angle=90, code=3)
}

```


* Choose the "best"" parameter value
```{r best_model gbm, echo=TRUE}
model_best=model_values[1]

if(run.cv){
  model_best <- model_values[which.min(err_cv[,1]),]
}

par_best <- list(depth=model_best$depth, n.trees=model_best$n.trees)
par_best

PSNR_gbm <- 20*log10(1) - 10 * log10(err_cv[which.min(err_cv[,1]),1])

```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train gbm}
tm_train=NA
tm_train <- system.time(fit_train <- train(feat_train, label_train, par_best))
save(fit_train, file="../output/fit_train.RData")
tm_train
```



### Model 2: XGBoost

#### Model selection with cross-validation (XGBoost)
* Do model selection by choosing among different values of training model parameters, that is, the interaction depth for GBM in this example. 
```{r runcv XGB, message=FALSE, warning=FALSE}
source("../lib/cross_validation_xgb.R")
source("../lib/train.xgboost.R")


if(run.cv){
  err_cv_xgb <- array(dim=c(nrow(model_values_xgb), 2))
  for(k in 1:nrow(model_values_xgb)){
    cat("k=", k, "\n")
    err_cv_xgb[k,] <- cv.function.xgb(feat_train, label_train, model_values_xgb[k, ], K)
  }
  save(err_cv_xgb, file="../output/err_cv_xgb.RData")
}
err_cv_xgb
```


Visualize cross-validation results. 
```{r cv_vis xgb}

plot_ly(x=model_values_xgb$max_depth, y=model_values_xgb$eta, z=err_cv.xgb, type="surface")
```

* Choose the "best"" parameter value
```{r best_model xgb}
model_best_xgb=model_values_xgb[1]
if(run.cv){
  model_best_xgb <- model_values_xgb[which.min(err_cv_xgb[,1]),]
}

par_best_xgb <- list(max_depth=model_best_xgb$max_depth, eta=model_best_xgb$eta, subsample = model_best_xgb$subsample, min_child_weight = model_best_xgb$min_child_weight)
#par_best_xgb <- list()

PSNR_xgb <- 20*log10(1) - 10 * log10(err_cv_xgb[which.min(err_cv_xgb[,1]),1])
PSNR_xgb
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train xgb, echo=TRUE, warning=FALSE}
#tm_train_xgb=NA
tm_train_xgb <- system.time(fit_train_xgb <- train.xgboost(feat_train, label_train, par_best_xgb))
save(fit_train_xgb, file="../output/fit_train_xgb.RData")

```



### Step 5: Super-resolution for test images
Feed the final training model with the completely holdout testing data. 
+ `superResolution.R`
  + Input: a path that points to the folder of low-resolution test images.
  + Input: a path that points to the folder (empty) of high-resolution test images.
  + Input: an R object that contains tuned predictors.
  + Output: construct high-resolution versions for each low-resolution test image.
```{r superresolution, echo=TRUE}
setwd("~/Documents/GitHub/Fall2018-Proj3-Sec1-grp6")

source("../lib/Superresolution.Parallel.R")
test_dir <- "./data/test_sample/" # This will be modified for different data sets.
test_LR_dir <- paste(test_dir, "LR/", sep="")
test_HR_dir_GBM <- paste(test_dir, "HR/GBM", sep="")


# Output HR versions using GBM
tm_test=NA
if(run.test){
  load(file="./output/fit_train.RData")
  tm_test <- system.time(superResolution.par(LR_dir=test_LR_dir, HR_dir=test_HR_dir_GBM, modelList=fit_train))
}
```

```{r superresolution XGB, echo=TRUE}
test_HR_dir_XGB <- paste(test_dir, "HR/XGB/", sep="")

# Output HR versions using XGBoost
tm_xgb_test=NA
if(run.test){
  load(file="../output/fit_train_xgb.RData")
  tm_test_xgb <- system.time(superResolution.par(LR_dir=test_LR_dir, HR_dir=test_HR_dir_XGB, modelList=fit_train_xgb))
}

```

### Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited.


```{r running_time}
cat("Time for constructing training features=", tm_feature_train[3], "s \n")
cat("Time for training GBM model=", tm_train[3], "s \n")
cat("Time for GBM super-resolution=", tm_test[3], "s \n")
cat("Time for training XGBoost model=", tm_train_xgb[3], "s \n")
cat("Time for XGBoost super-resolution=", tm_test_xgb[3], "s \n")
```

