---
title: "Project 3"
author: "Group 3 Yunfan Li, Mingyu Yang, Peiqi Jin, Shichao Jia, Yanchen Chen"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
---


### Load required packages
```{r}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
}

if(!require("gbm")){
  install.packages("gbm")
}

library("EBImage")
library("gbm")

if(!require("xgboost")){
  install.packages("xgboost")
}

if(!require("plotly")){
  install.packages("plotly")
}

library("EBImage")
library("gbm")
library("xgboost")
library("plotly")

```


### Step 0: specify directories.

Set the working directory to the image folder.In order to obain reproducible results, set.seed() whenever randomization is used. 

```{r wkdir, eval=FALSE}
set.seed(2018)
setwd("C:/43/paraalel/Fall2018-Proj3-Sec1-grp6")

```


Provide directories for training images. Low-resolution (LR) image set and High-resolution (HR) image set will be in different subfolders. 
```{r}
train_dir <- "../data/train_set/" # This will be modified for different data sets.

train_LR_dir <- paste(train_dir, "LR/", sep="")
train_HR_dir <- paste(train_dir, "HR/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="") 

```

### Step 1: set up controls for evaluation experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

```{r exp_setup}
run.cv=TRUE # run cross-validation on the training set
K <- 3  # number of CV folds
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
```

Using cross-validation or independent test set evaluation, we compare the performance of models with different specifications. In the baseline, we use GBM with different `depth` and `trees`.In the advanced model, we use XGBoost with different `max_depth`, `eta`, `subsample`, `min_child_weight`. 
In the following chunk,we list, `depth` and `trees` corresponding to GBM model that we will compare.
Also, we list `max_depth`, `eta`, `subsample`, `min_child_weight` corresponding to Xgboost model that we will compare.

```{r model_setup}

model_values <- expand.grid(depth = c(3,13), n.trees = c(150,300))

model_values_xgb <- expand.grid(max_depth = c(4,6,8), eta = c(0.3,0.5), subsample = c(0.5,0.8), min_child_weight = c(4,6,8))


```


### Step 2: construct features and responses

`feature.R` wraps for feature engineering functions and options. 

```{r feature}
#setwd("~/Documents/GitHub/Fall2018-Proj3-Sec1-grp6")
source("../lib/feature.R")

tm_feature_train <- NA
if(run.feature.train){
  tm_feature_train <- system.time(dat_train <- feature(train_LR_dir, train_HR_dir))
  feat_train <- dat_train$feature
  label_train <- dat_train$label
}

```


### Step 3: Train a classification model with training images 
Call the train model and test model from library. 

Model1: GBM


```{r loadlib}
library("doParallel")
library("foreach")
source("../lib/train.gbmpar.R")
source("../lib/test.R")
source("../lib/train.xgboost.R")
```

#### Model selection with cross-validation (GBM)
* Do model selection by choosing among different values of training model parameters, that is, the interaction depth and n.tress for GBM 
```{r runcv, message=FALSE, warning=FALSE}
source("../lib/cross_validation_gbmpar.R")

if(run.cv){
  err_cv.gbmpar <- array(dim=c(nrow(model_values), 2))
  for(k in 1:nrow(model_values)) {
    cat("k=", k, "\n")
    err_cv.gbmpar[k,] <- cv.function.gbmpar(feat_train, label_train, model_values[k, ], K)
  }
  save(err_cv.gbmpar, file="../output/err_cv.gbmpar.RData")
}

err_cv.gbmpar



```

Visualize cross-validation results. 
```{r cv_vis}

print(Sys.time())

if(run.cv){
  load("../output/err_cv.gbmpar.RData")
  mv <- factor(paste("(", model_values$depth,",", model_values$n.trees, ")"))
  plot(err_cv.gbmpar[,1]~mv, xlab="(Interaction Depth, Number of trees)", ylab="CV Error",
       main="Cross Validation Error", type="n", ylim=c(0, 0.005))
  points(err_cv.gbmpar[,1]~mv, col="blue", pch=16)
  lines(err_cv.gbmpar[,1]~mv, col="blue")
  #arrows(mv,err_cv[,1]-err_cv[,2], mv, err_cv[,1]+err_cv[,2], 
        #length=0.1, angle=90, code=3)
}

```


* Choose the "best"" parameter value
```{r best_model}
model_best=model_values[1]
if(run.cv){
  model_best <- model_values[which.min(err_cv.gbmpar[,1]),]
}

par_best <- list(depth=model_best$depth, n.trees=model_best$n.trees)
par_best

PSNR_gbm <- 20*log10(1) - 10 * log10(err_cv.gbmpar[which.min(err_cv.gbmpar[,1]),1])
PSNR_gbm

```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train}
tm_train_gbm=NA
tm_train_gbm <- system.time(fit_train.gbmpar <- train.gbmpar(feat_train, label_train, par_best))
save(fit_train.gbmpar, file="../output/fit_train.gbmpar.RData")
```



Model 2: XGBoost

#### Model selection with cross-validation (XGBoost)
* Do model selection by choosing among different values of training model parameters, that is, the interaction max_depth, eta, subsample, min_child_weight in this example. 
```{r runcv_xgb, message=FALSE, warning=FALSE}

source("../lib/cross_validation_xgb.R")

if(run.cv){
  err_cv.xgb <- array(dim=c(nrow(model_values_xgb), 2))
  for(k in 1:nrow(model_values_xgb)){
    cat("k=", k, "\n")
    err_cv.xgb[k,] <- cv.function.xgb(X.train=feat_train, y.train=label_train, modelvalues=model_values_xgb[k, ], K=K)
  }
  save(err_cv.xgb, file="../output/err_cv.xgb.RData")
}

err_cv.xgb
```


Visualize cross-validation results. 
```{r cv_vis_xgb}
plot_ly(x=model_values_xgb$max_depth, y=model_values_xgb$eta, z=err_cv.xgb, type="surface")
```

* Choose the "best"" parameter value
```{r best_model_xbg}
model_best_xgb=model_values_xgb[1]
if(run.cv){
  model_best_xgb <- model_values_xgb[which.min(err_cv.xgb[,1]),]
}

par_best_xgb <- list(max_depth=model_best_xgb$max_depth, eta=model_best_xgb$eta, subsample = model_best_xgb$subsample, min_child_weight = model_best_xgb$min_child_weight)
#par_best_xgb <- list()

PSNR_xgb <- 20*log10(1) - 10 * log10(err_cv.xgb[which.min(err_cv.xgb[,1]),1])
PSNR_xgb
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train_xgb}
tm_train_xgb=NA
tm_train_xgb <- system.time(fit_train.xgb <- train.xgboost(feat_train, label_train, par_best_xgb))
save(fit_train.xgb, file="../output/fit_train_xgb.RData")

```



### Step 4: Super-resolution for test images
Feed the final training model with the completely holdout testing data. 
+ `superResolution.R`
  + Input: a path that points to the folder of low-resolution test images.
  + Input: a path that points to the folder (empty) of high-resolution test images.
  + Input: an R object that contains tuned predictors.
  + Output: construct high-resolution versions for each low-resolution test image.
```{r superresolution_xgb}
setwd("C:/43/paraalel/Fall2018-Proj3-Sec1-grp6")
source("./lib/Superresolution.Parallel.R")
source("./lib/test.R")
test_dir <- "./data/test_sample/" # This will be modified for different data sets.
test_LR_dir <- paste(test_dir, "LR/", sep="")
test_HR_dir_GBM <- paste(test_dir, "HR/GBM/", sep="")
test_HR_dir_XGB <- paste(test_dir, "HR/XGB/", sep="")


# Output HR versions using GBM
tm_test_gbm=NA
if(run.test){
  load(file="./output/fit_train.gbmpar.RData")
  tm_test_gbm <- system.time(superResolution.par(LR_dir=test_LR_dir, HR_dir=test_HR_dir_GBM, modelList=fit_train.gbmpar))
}

# Output HR versions using XGBoost
tm_xgb_test=NA
if(run.test){
  load(file="./output/fit_train_xgbpar.RData")
  tm_test_xgb <- system.time(superResolution.par(LR_dir=test_LR_dir, HR_dir=test_HR_dir_XGB, modelList=fit_train.xgb))
}

```

### Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time_xgb}
cat("Time for constructing training features=", tm_feature_train[1], "s \n")
#cat("Time for constructing testing features=", tm_feature_test[1], "s \n")
cat("Time for training GBM model=", tm_train_gbm[1], "s \n")
cat("Time for GBM super-resolution=", tm_test_gbm[1], "s \n")
cat("Time for training XGBoost model=", tm_train_xgb[1], "s \n")
cat("Time for XGBoost super-resolution=", tm_test_xgb[1], "s \n")
```

